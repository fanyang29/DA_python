################################
#############1.introduction to sampling 
#1. sampling and point estimate use .sample()
book= book.sample(n=10)
book1= book["name"].sample(n=10)

#population parameters & point estimates
book_sample_mean= book["price"].mean()

coffee_ratings["total_cup_points"].hist(bins=np.arange(59,93,2))  # the stop value is exclusive  so the range is actually 59,91, but we need to make it 93

# pseudo-random number generation
seed=1
calc_next_random(seed)

np.random.beta()
np.random.binomial()
np.random.chisquare()

 # set a random seed
np.random.seed(1)
np.random.normal(loc=2, scale=1.5, size=2)   #loc: mean, sclae: std size: how many random numebrs will be returned





#######################
##### 2.Sampling methods
#1.simple random and systematic sampling
# simple random sampling with pandas
book.sample(n=5, random_state=123456)   # random_state is seed

# systematic sampling. defining the interval,determining how big the interval between each row should be for a given sample size.
sample_size=5
pop_size=len(coffee_ratings)
print(pop_size)
interval= pop_size//sample_size
print(interval)  #select every 267 row

coffee_rating.iloc[::interval]   ## To select every two hundred and sixty-seventh row, we call dot-iloc on coffee_ratings and pass double-colons and the interval, which is 267 in this case. Double-colon interval tells pandas to select every two hundred and sixty-seventh row from zero to the end of the DataFrame

# the trouble with systematic sampling:
#Suppose we are interested in statistics about the aftertaste attribute of the coffees. To examine this, first, we use reset_index to create a column of index values in our DataFrame that we can plot. Plotting aftertaste against index shows a pattern. Earlier rows generally have higher aftertaste scores than later rows. This introduces bias into the statistics that we calculate. In general, it is only safe to use systematic sampling if a plot like this has no pattern; that is, it just looks like noise.
coffee_ratings_with_id=coffee_ratins.reset_index()   #create a column of index values
coffee_ratings_with_id.plot(x="index", y="aftertaste", kind="scatter")
plt.show()  #we can only use systamtic sampling if we see the plot is random like noise, if there's pattern (e.g. higher index with higher aftertaste) we can not use sysSamp

#To To ensure that systematic sampling is safe, we can randomize the row order before sampling. 
shuffled=coffee_ratings.sample(frac=1)   #randomly 
shuffled= shuffled.reset_index(drop=True).reset_index()
shuffled.plot(x="index", y="aftertaste", kind="scatter")

#2. Stratified and weighted random sampling
top_counted_countries=["Mexico","Columbia","Taiwan"]
top_counted_subset=coffee_ratings["country_of_origin"].isin(top_counted_countries)   #filter the population and only return the rows corresponding to the selected countries
coffee_ratings_top=coffee_ratings[top_counted_subset]
coffee_ratings_sample=coffee_ratings_top.sample(frac=0.1, random_state=2021) # take a ten percent simple random sample of the dataset using .sample with frac =0.1
coffee_ratings_sample["country_of_origin"].value_counts(normalize=True) #look at the counts for each country (proportion)
  
   #Proportional stratified sampling
coffee_ratings_strat= coffee_rating_top.groupby("country_of_origin")\
              .sample(frac=0.1, random_state=2021)    #\ is to break long sentence

coffee_ratings_strat["country_of_origin"].value_counts(normalize=True)  # now the porportion is much more closer to those in the population

  # sample equal counts from each group
coffee_ratings_eq= coffee_ratings_top.groupby("country_of_origin")\
                  .sample(n=15, random_state=2021)


# Weighted random sampling
 #provides even more flexibility;specify weights to adjust the relative probability of a row being sampled
 #suppose we thing it was important to have a higher porpotion of Taiwanese coffees in the sample than in the populatin
import numpy as np
coffee_ratings_weights= coffee_ratings_top
condition=coffee_ratings_weight["country of origin"] =="Taiwan"
coffee_ratings_weight["weight"]=np.where(condition,2,1)  #This means when each row is randomly sampled, Taiwanese coffees have two times the chance of being picked compared to other coffees.
coffee_ratings_weight= coffee_ratings_weight.sample(frac=0.1, weights="weight")
  #plot hist with bins of width 1 from 0 to 40
attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))

# Sample 400 employees weighted by YearsAtCompany
attrition_weight = attrition_pop.sample(n=400, weights="YearsAtCompany")


# 3. CLuster sampling
 #The stratified sampling approach was to 
   #a. split the population into subgroups, 
   #b. then use simple random sampling on each of them.   Cluster sampling means that we limit the number of subgroups in the analysis by picking a few of them with simple random sampling. We then perform simple random sampling on each subgroup as before.

 # first, list all unique subgroups
varieties_pop=list(coffee_ratings["variety"].unique())
import random
varieties_samp= random.sample(varieties_pop, k=3)
 #second, sampling each group
variety_condition=coffee_ratings["variety"].isin(varieties_samp)
coffee_ratings_cluster=coffee_ratings[variety_condition]  #Filter for rows where variety is in varieties_samp

# To ensure that the isin filtering removes levels with zero rows, we apply the cat-dot-remove_unused_categories method on the Series of focus, which is variety here.
coffee_ratings_cluster["variety"]=coffee_ratings_cluster["variety"].cat.remove_unused_categories()

coffee_ratings_sample= coffee_ratings_cluster["variety"].sample(n=15)
  #or
# Perform cluster sampling on the selected group, getting 0.25 of attrition_pop
attrition_clust = attrition_clust_prep.groupby("RelationshipSatisfaction")\
    .sample(n=len(attrition_pop) // 4, random_state=2022)
# We can also do multistage sampling: random clusters and random pick 

# 4. Comparing different sampling 
   
 #Simple: Mean Attrition by RelationshipSatisfaction group
mean_attrition_pop = attrition_pop.groupby("RelationshipSatisfaction")["Attrition"].mean()

# Print the result
print(mean_attrition_pop)


################################
#############3.Sampling distribution

# 1. Relative error of point estimates 
 # how sample size affect accuracy of point estimates
len(coffee_ratings.sample(300))

#Relative errors
# Population parameter:
population_mean=coffee_ratings["total_cup_points"].mean()
#point estimate
sample_mean=coffee_ratings.sample(n=sample_size)["total_cup_points"].mean()
# relative error: the absolute difference between the two numbers
rel_error_pct=100*abs(population_mean-sample_mean)/population_mean

# 2. Creating a sampling distribution
  # for loop

mean_cup_points_1000=[]   #We start by creating an empty list to store the means. 
for i in range(1000):
    mean_cup_points_1000.append(
        coffee_ratings.sample(n=30)["total_cup_points"].mean()
    )
print(mean_cup_points_1000)
plt.hist(mean_cup_points_1000, bins=30)
#A distribution of replicates of sample means, or other point estimates, is known as a sampling distribution.

# 3. approximate sampling distributions

  #4 dice
dice= expand_grid(
    {'die1': [1,2,3,4,5,6],
     'die2': [1,2,3,4,5,6],
     'die3': [1,2,3,4,5,6],
     'die4': [1,2,3,4,5,6]}
)

  #mean roll by adding mean_roll
dice['mean_roll'] = (dice['die1']+dice['die2']+dice['die3']+dice['die4'])/4
# the mean roll takes discrete values, so we use barplot
dice['mean_roll']= dice['mean_roll'].astype('category')
dice['mean_roll'].value_counts(sort=False).plot(kind="bar")    #exact sampling 

# if we increase the number of dice in our scenario
n_dice=list(range(1,101))
n_outcomes=[]
for n in n_dice:
    n_outcomes.append(6**n)

outcomes= pd.DataFrame(
    {"n_dice": n_dice,
    "n_outcomes": n_outcomes}
)
outcomes.plot(x="n_dice",
             y="n_outcomes",
             kind="scatter")
plt.show()

# simulating the mean of four dice rolls, 4 dice, range(1,7) means number 1-6, then use loop to generate 100 sample means
import numpy as np
sample_means_1000= []
for i in range(1000):
    sample_means_1000.append(
  np.random.choice(list(range(1,7)), size=4, replace=True).mean())   # approximate sampling


#  Standard errors and the Central Limit Theorem

#Consequences of the central limit theorem

#what the central limit theorem tells us. The means of independent samples have normal distributions. Then, as the sample size increases, we see two things. The distribution of these averages gets closer to being normal, and the width of this sampling distribution gets narrower.

coffee_ratings['total_cup_points'].mean()
coffee_ratings['total_cup_points'].std(ddof=0)     #ddof=0: population std, ddof=1: sample std

# standard error: standard deviation of the sampling distribution
#The standard deviation of the sampling distribution is approximately equal to the population standard deviation divided by the square root of the sample size. 

