################################
#############1.introduction to sampling 
#1. sampling and point estimate use .sample()
book= book.sample(n=10)
book1= book["name"].sample(n=10)

#population parameters & point estimates
book_sample_mean= book["price"].mean()

coffee_ratings["total_cup_points"].hist(bins=np.arange(59,93,2))  # the stop value is exclusive  so the range is actually 59,91, but we need to make it 93

# pseudo-random number generation
seed=1
calc_next_random(seed)

np.random.beta()
np.random.binomial()
np.random.chisquare()

 # set a random seed
np.random.seed(1)
np.random.normal(loc=2, scale=1.5, size=2)   #loc: mean, sclae: std size: how many random numebrs will be returned





#######################
##### 2.Sampling methods
#1.simple random and systematic sampling
# simple random sampling with pandas
book.sample(n=5, random_state=123456)   # random_state is seed

# systematic sampling. defining the interval,determining how big the interval between each row should be for a given sample size.
sample_size=5
pop_size=len(coffee_ratings)
print(pop_size)
interval= pop_size//sample_size
print(interval)  #select every 267 row

coffee_rating.iloc[::interval]   ## To select every two hundred and sixty-seventh row, we call dot-iloc on coffee_ratings and pass double-colons and the interval, which is 267 in this case. Double-colon interval tells pandas to select every two hundred and sixty-seventh row from zero to the end of the DataFrame

# the trouble with systematic sampling:
#Suppose we are interested in statistics about the aftertaste attribute of the coffees. To examine this, first, we use reset_index to create a column of index values in our DataFrame that we can plot. Plotting aftertaste against index shows a pattern. Earlier rows generally have higher aftertaste scores than later rows. This introduces bias into the statistics that we calculate. In general, it is only safe to use systematic sampling if a plot like this has no pattern; that is, it just looks like noise.
coffee_ratings_with_id=coffee_ratins.reset_index()   #create a column of index values
coffee_ratings_with_id.plot(x="index", y="aftertaste", kind="scatter")
plt.show()  #we can only use systamtic sampling if we see the plot is random like noise, if there's pattern (e.g. higher index with higher aftertaste) we can not use sysSamp

#To To ensure that systematic sampling is safe, we can randomize the row order before sampling. 
shuffled=coffee_ratings.sample(frac=1)   #randomly 
shuffled= shuffled.reset_index(drop=True).reset_index()
shuffled.plot(x="index", y="aftertaste", kind="scatter")

#2. Stratified and weighted random sampling
top_counted_countries=["Mexico","Columbia","Taiwan"]
top_counted_subset=coffee_ratings["country_of_origin"].isin(top_counted_countries)   #filter the population and only return the rows corresponding to the selected countries
coffee_ratings_top=coffee_ratings[top_counted_subset]
coffee_ratings_sample=coffee_ratings_top.sample(frac=0.1, random_state=2021) # take a ten percent simple random sample of the dataset using .sample with frac =0.1
coffee_ratings_sample["country_of_origin"].value_counts(normalize=True) #look at the counts for each country (proportion)
  
   #Proportional stratified sampling
coffee_ratings_strat= coffee_rating_top.groupby("country_of_origin")\
              .sample(frac=0.1, random_state=2021)    #\ is to break long sentence

coffee_ratings_strat["country_of_origin"].value_counts(normalize=True)  # now the porportion is much more closer to those in the population

  # sample equal counts from each group
coffee_ratings_eq= coffee_ratings_top.groupby("country_of_origin")\
                  .sample(n=15, random_state=2021)


# Weighted random sampling
 #provides even more flexibility;specify weights to adjust the relative probability of a row being sampled
 #suppose we thing it was important to have a higher porpotion of Taiwanese coffees in the sample than in the populatin
import numpy as np
coffee_ratings_weights= coffee_ratings_top
condition=coffee_ratings_weight["country of origin"] =="Taiwan"
coffee_ratings_weight["weight"]=np.where(condition,2,1)  #This means when each row is randomly sampled, Taiwanese coffees have two times the chance of being picked compared to other coffees.
coffee_ratings_weight= coffee_ratings_weight.sample(frac=0.1, weights="weight")
  #plot hist with bins of width 1 from 0 to 40
attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))

# Sample 400 employees weighted by YearsAtCompany
attrition_weight = attrition_pop.sample(n=400, weights="YearsAtCompany")


# 3. CLuster sampling
 #The stratified sampling approach was to 
   #a. split the population into subgroups, 
   #b. then use simple random sampling on each of them.   Cluster sampling means that we limit the number of subgroups in the analysis by picking a few of them with simple random sampling. We then perform simple random sampling on each subgroup as before.

 # first, list all unique subgroups
varieties_pop=list(coffee_ratings["variety"].unique())
import random
varieties_samp= random.sample(varieties_pop, k=3)
 #second, sampling each group
variety_condition=coffee_ratings["variety"].isin(varieties_samp)
coffee_ratings_cluster=coffee_ratings[variety_condition]  #Filter for rows where variety is in varieties_samp

# To ensure that the isin filtering removes levels with zero rows, we apply the cat-dot-remove_unused_categories method on the Series of focus, which is variety here.
coffee_ratings_cluster["variety"]=coffee_ratings_cluster["variety"].cat.remove_unused_categories()

coffee_ratings_sample= coffee_ratings_cluster["variety"].sample(n=15)
  #or
# Perform cluster sampling on the selected group, getting 0.25 of attrition_pop
attrition_clust = attrition_clust_prep.groupby("RelationshipSatisfaction")\
    .sample(n=len(attrition_pop) // 4, random_state=2022)
# We can also do multistage sampling: random clusters and random pick 

# 4. Comparing different sampling 
   
 #Simple: Mean Attrition by RelationshipSatisfaction group
mean_attrition_pop = attrition_pop.groupby("RelationshipSatisfaction")["Attrition"].mean()

# Print the result
print(mean_attrition_pop)


################################
#############3.Sampling distribution

# 1. Relative error of point estimates 
 # how sample size affect accuracy of point estimates
len(coffee_ratings.sample(300))

#Relative errors
# Population parameter:
population_mean=coffee_ratings["total_cup_points"].mean()
#point estimate
sample_mean=coffee_ratings.sample(n=sample_size)["total_cup_points"].mean()
# relative error: the absolute difference between the two numbers
rel_error_pct=100*abs(population_mean-sample_mean)/population_mean

# 2. Creating a sampling distribution
  # for loop

mean_cup_points_1000=[]   #We start by creating an empty list to store the means. 
for i in range(1000):
    mean_cup_points_1000.append(
        coffee_ratings.sample(n=30)["total_cup_points"].mean()
    )
print(mean_cup_points_1000)
plt.hist(mean_cup_points_1000, bins=30)
#A distribution of replicates of sample means, or other point estimates, is known as a sampling distribution.

# 3. approximate sampling distributions

  #4 dice
dice= expand_grid(
    {'die1': [1,2,3,4,5,6],
     'die2': [1,2,3,4,5,6],
     'die3': [1,2,3,4,5,6],
     'die4': [1,2,3,4,5,6]}
)

  #mean roll by adding mean_roll
dice['mean_roll'] = (dice['die1']+dice['die2']+dice['die3']+dice['die4'])/4
# the mean roll takes discrete values, so we use barplot
dice['mean_roll']= dice['mean_roll'].astype('category')
dice['mean_roll'].value_counts(sort=False).plot(kind="bar")    #exact sampling 

# if we increase the number of dice in our scenario
n_dice=list(range(1,101))
n_outcomes=[]
for n in n_dice:
    n_outcomes.append(6**n)

outcomes= pd.DataFrame(
    {"n_dice": n_dice,
    "n_outcomes": n_outcomes}
)
outcomes.plot(x="n_dice",
             y="n_outcomes",
             kind="scatter")
plt.show()

# simulating the mean of four dice rolls, 4 dice, range(1,7) means number 1-6, then use loop to generate 100 sample means
import numpy as np
sample_means_1000= []
for i in range(1000):
    sample_means_1000.append(
  np.random.choice(list(range(1,7)), size=4, replace=True).mean())   # approximate sampling


#  Standard errors and the Central Limit Theorem

#Consequences of the central limit theorem

#what the central limit theorem tells us. The means of independent samples have normal distributions. Then, as the sample size increases, we see two things. The distribution of these averages gets closer to being normal, and the width of this sampling distribution gets narrower.

coffee_ratings['total_cup_points'].mean()
coffee_ratings['total_cup_points'].std(ddof=0)     #ddof=0: population std, ddof=1: sample std

# standard error: standard deviation of the sampling distribution
#The standard deviation of the sampling distribution is approximately equal to the population standard deviation divided by the square root of the sample size. 



###############4 Bootstrap Distributions ###################

  #1. Introduction to bootstrapping

coffee_focus=coffee_ratings[["variety","country", "flavor"]]
coffee_focus=coffee_focus.reset_index()

 # resampling with.sample()
coffee_resamp=coffee_focus.sample(frac=1, replace=True)
coffee_resamp["index"].value_counts()  #shows how many times each coffee ended up in the resampled dataset

num_unique_coffees= len(coffee_resamp.drop_duplicates(subset="index"))   # see the unique coffees were included

len(coffee_ratings)- num_unique_coffees

# With sampling, we treat the dataset as the population and move to a smaller sample. With bootstrapping, we treat the dataset as a sample and use it to build up a theoretical population. A use case of bootstrapping is to try to understand the variability due to sampling. This is important in cases where we aren't able to sample the population multiple times to create a sampling distribution.

# Process
  #1. randomly sample with replacement to get a resample the same size as the original dataset. 
  #2. Then, calculate a statistic, such as a mean of one of the columns. 
  #3. repeat steps 1 and 2

import numpy as np
mean_flavor_1000=[]
for i in range(1000):
    mean_flavor_1000.append(
        coffee_sample.sample(frac=1, replace=True)['flavor'].mean())
    )
plt.hist(mean_flavor_1000)
#Radical replacement reasoning! The key to deciding whether to sample without or with replacement is whether or not your dataset is best thought of as being the whole population or not.


# 2. Comparing sampling and bootstrap distributions
#l. The bootstrap distribution mean is usually almost identical to the original sample mean. However, that is not often a good thing. If the original sample wasn't closely representative of the population, then the bootstrap distribution mean won't be a good estimate of the population mean. Bootstrapping cannot correct any potential biases due to differences between the sample and the population.

 # Although bootstrapping was poor at calculating the populaiton mean, The estimated standard error times the square root of the sample size gives a really good estimate of the standard deviation of the population. 



# Confidence interval
#CI: "values within one standard deviation of the mean", which gives a good sense of where most of the values in a distribution lie. 
# CI account for uncertainty in our estimate of a population parameter by providing a range of possible values. We are confident that the true value lies somewhere in the interval specified by that range.
 
 # quantile method for CI
np.quantile(coffee_bot_distn, 0.025)
np.quantile(coffee_bot_distn, 0.975)

# inverse cumulative distribution fucntion
# PDF: bell curve we've seen before is the probability density function or PDF. 
# CDF:  Using calculus, if we integrate this, we get the cumulative distribution function or CDF. 
# inverse CDF: If we flip the x and y axes, we get the inverse CDF. 
from scipy.stats import norm
norm.ppf(quantile, loc=0, scale=1)

point_estimate=np.mean(coffee_bot_distn)
std_error=np.std(coffee_bot_distn, ddof=1)
lower= norm.ppf(0.025, loc=point_estimate, scale= std_error)
upper= norm.ppf(0.975, loc=point_estimate, scale= std_error)
