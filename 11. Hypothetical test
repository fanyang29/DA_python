###################################
#########1. Hypothesis tests and z-scores
  #1. generating a bootstrap distribution
import numpy as np
# Step3. Repeat steps1&2 many times, appending to a list
so_boot_distn=[]
for i in range(5000):
  so_boot_distn.append(
  #Step 2. Calcualte point estimate
   np.mean(
      # Step 1. Resample
stack_overflow.sample(frac=1, replace=True)['converted_temp']
   )
)


# get Std
std_error=np.std(so_boot_distn, ddof=1)
 #z score = (sample stat - mean)/std


# Calculate the proportion of late shipments
late_prop_samp = (late_shipments['late']=="Yes").mean()


#calculate p-value
# left-tail test-> norm.cdf() , right-tail test 1-norm.cdf()

from scipy.stats import norm
1-norm.cdf(z_score, loc=0, scale=1)



####################################
###########2. performing t-tests
 # 1. calculate groupwise summary statistics
stack_over.groupby("age")["coverted_comp"].mean()
stack_over.groupby("age")["coverted_comp"].std()
stack_over.groupby("age")["coverted_comp"].count()
# t=((x1mean -x2mean)-(miu1-miu2))/ SE(x1-x2)
# SE(x1samplemean -x2sample mean)= sqrt(s1^2/n1 + s2^2/n2) 

# Z-statistic: needed when using one sample statistic to estimate a population parameter
# t-statistic: needed when using multiple sample statistics to estimate a population parameter

degree_of_freedom= n1+n2-2

1-t.cdf(d_stat, df=degree_of_freedom)


2. # ttest
pingouin.ttest(x=sample_data["rep_08"], y=sample_data["rep_12"], paired=False, alternative="less")


#3. Anova test: test differences between groups
alpha=0.2
pingouin.anova(data=stack,
              dv= "converted_comp",
              between="job_sat")    
  # pairwise tests
pingouin.pairwise_tests(ata=stack,
              dv= "converted_comp",
              between="job_sat",
              padjust="none")
 #3 of these have pvalues less than 0.2, In this case we have five groups, resulting in ten pairs. As the number of groups increases, the number of pairs - and hence the number of hypothesis tests we must perform - increases quadratically. The more tests we run, the higher the chance that at least one of them will give a false positive significant result. With a significance level of 0.2, if we run 1 test, the chance of a false positive result is 0.2. With 5 groups and 10 tests, the probability of at least 1 false positive is around 0.7. With twenty groups, it's almost guaranteed that we'll get at least 1 false positive. 
 
# The solution to this is to apply an adjustment to increase the p-values, reducing the chance of getting a false positive. One common adjustment is the Bonferroni correction. Looking at the p-corr column corresponding to corrected p-values, as opposed to the p-unc column for uncorrected, only two of the pairs appear to have significant differences.
pingouin.pairwise_tests(ata=stack,
              dv= "converted_comp",
              between="job_sat",
              padjust="bonf")


#######################################
######3. One-sample proportion tests
# The hypothesis tests in Chapter 1 measured whether or not an unknown population proportion was equal to some value. We used bootstrapping on the sample to estimate the standard error of the sample statistic. The standard error was then used to calculate a standardized test statistic, the z-score, which was used to get a p-value, so we could decide whether or not to reject the null hypothesis. A bootstrap distribution can be computationally intensive to calculate, so this time we'll instead calculate the test statistic without it.

numerator= p_hat-p_0
denominator=np.sqrt(p_0*(1-p_0)/n)
z_score=numerator/denominator)
p_value= norm.cdf(z_score)  #left tail
p_value= 1- norm.cdf(z_score)   #right tail
p_value=norm.cdf(-z_score) + 1- norm.cdf(z_score)  # two-tailed
p_value= 2*(1-norm.cdf(z_score) )

 #example
# Hypothesize that the proportion of late shipments is 6%
p_0 = 0.06

# Calculate the sample proportion of late shipments
p_hat = (late_shipments['late'] == "Yes").mean()

# Calculate the sample size
n = len(late_shipments)

# Calculate the numerator and denominator of the test statistic
numerator = p_hat - p_0
denominator = np.sqrt(p_0 * (1 - p_0) / n)

# Calculate the test statistic
z_score = numerator / denominator

# Calculate the p-value from the z-score
p_value = 1- norm.cdf(z_score)

# Print the p-value
print(p_value)


##2. Two-sample proportion tests (H0: the proportion of hobbyist users is the same for the under thirty age category as the thirty or over category)

# use proportions_ztest()
stack_overflow.groupby("age_cat")["hobbyist"].value_counts()
n_hobbyists=np.array([812, 1021])
n_rows=np.array([812+238, 1021+190])

from statsmodels.stats.proportion import proportion_ztest
z_score, p_value=proportion_ztest(count=n_hobbyists, nobs=n_rows, alternative="two-sided")  #return a z-score and pvalue

# example- traditional way
# Calculate the pooled estimate of the population proportion
p_hat = (p_hats["reasonable"] * ns["reasonable"] + p_hats["expensive"] * ns["expensive"]) / (ns["reasonable"] + ns["expensive"])

# Calculate p_hat one minus p_hat
p_hat_times_not_p_hat = p_hat * (1 - p_hat)

# Divide this by each of the sample sizes and then sum
p_hat_times_not_p_hat_over_ns = p_hat_times_not_p_hat / ns["expensive"] + p_hat_times_not_p_hat / ns["reasonable"]

# Calculate the standard error
std_error = np.sqrt(p_hat_times_not_p_hat_over_ns)

# Calculate the z-score
z_score = (p_hats["expensive"] - p_hats["reasonable"]) / std_error

# Calculate the p-value from the z-score
p_value = 1-norm.cdf(z_score)

# Print p_value
print(p_value)



# example using proportions_ztest()
# Count the late column values for each freight_cost_group
late_by_freight_cost_group = late_shipments.groupby("freight_cost_group")['late'].value_counts()

# Create an array of the "Yes" counts for each freight_cost_group
success_counts = np.array([45, 16])

# Create an array of the total number of rows in each freight_cost_group
n = np.array([45+500, 16+439])

# Run a z-test on the two proportions
stat, p_value = proportions_ztest(count=  success_counts ,nobs=n, alternative="larger")


# Print the results
print(stat, p_value)




#3. Chi-square test of independence
  # statistical independent: when the proportion of successes in the response variable is the same across all categories of the explanatory variable.

expected, observed, stats= pingouin.chi2_independence(data=stack,
                                                     x="hobbyist",
                                                     y="age_cat",
                                                     correction=False)  #correction is a fudge factor for when the sample size is very small and the dof is 1
stack_overflow["age_cat"].value_counts()

    # Hypothesis0: Age categories are independent of job satisfaction levels
    # Chi2: Assuming independence, how far away are the observed results from the expected values
props=stack.groupby("job")["age"].value_counts(normalize=True)
wide_props=props.unstack()
wide_props.plot(kind="bar", stacked=True)

 #chi2 test
expected, observed, stats= pingouin.chi2_independence(data=stack,
                                                     x="job",
                                                     y="age",
                                                     correction=False)  #dof= (n.response_cat-1)*(n_explanatory_cat -1)

# Chi2 test tend to be right-tailed test

# EXAMPLE
# Proportion of freight_cost_group grouped by vendor_inco_term
props = late_shipments.groupby('vendor_inco_term')['freight_cost_group'].value_counts(normalize=True)

# Convert props to wide format
wide_props = props.unstack()

# Proportional stacked bar plot of freight_cost_group vs. vendor_inco_term
wide_props.plot(kind="bar", stacked=True)
plt.show()

# Determine if freight_cost_group and vendor_inco_term are independent
expected, observed, stats = pingouin.chi2_independence(data= late_shipments,
x="freight_cost_group",
y="vendor_inco_term")

# Print results
print(stats[stats['test'] == 'pearson']) 



# Chi-square goodness of fit test
  # compare a single categorical varaible to a hypothesized distribution

plt.bar(purpul_link_counts["purple_link"], purple_link_counts["n"],
       color="red", label="Observed")

chisquare(f_obs=purple_link_counts["n"], f_exp=hypothesized['n'])

# EXAMPLE
# Find the number of rows in late_shipments
n_total = len(late_shipments)

# Create n column that is prop column * n_total
hypothesized["n"] = hypothesized["prop"] * n_total

# Plot a red bar graph of n vs. vendor_inco_term for incoterm_counts
plt.bar(incoterm_counts['vendor_inco_term'], incoterm_counts['n'], color="red", label="Observed")

# Add a blue bar plot for the hypothesized counts
plt.bar(hypothesized['vendor_inco_term'],  
hypothesized["n"],
alpha=0.5, color="blue",
label="Hypothesized")
plt.legend()
plt.show()

# Perform a goodness of fit test on the incoterm counts n
gof_test = chisquare(f_obs= incoterm_counts["n"], f_exp=hypothesized["n"])




#####################################
###########4. Assumptions in hypothesis testing, non-parametric tests
  # 3 hypothetis: random sample, big enough(>=30), independent rows
# inspect whether the counts are "big enough >=30 " for a two sample t-test.
# inspect whether the counts are "big enough >=10 " for a one sample proportion test.
# inspect whether the counts are "big enough >=5 " for a chi-square independence test..
# inspect whether the counts are "big enough >=30 " for ANOVA test.

# Count the late values
counts = late_shipments["late"].value_counts()

# Print the result
print(counts)

# Inspect whether the counts are big enough
print((counts >= 10).all())


# Non-parametric tests

# non-parametric tests work better than the parametric alternative in situations where the sample size is small or the data cannot be assumed to be normally distributed.

 # 2ilcoxon-signed rank test, 1. take differences of pairs, 2, rank absolute differences 4. test-statistics (sum of ranks for negatvie and positive differences)

repub["diff"]= repub["rep_08"]- repub["rep_12"]
repub["abs_diff"]= repub["diff"].abs()
repub["rank_abs_diff"]= rankdata(repub["abs_diff"])
t_minus=1+4+5+2+3
t_plus=0
W=np.min([t_minus, t_plus])

pingouin.wilcoxon(x=repub["repub_08"], y=repub["repub_12"], alternative="less")


# Non-parametric ANOVA and unpaired ttests - Wilcoxon .mann-Whitney

age_vs_comp=stack[["converted_comp", "age_first_code_cud"]]
age_vs_comp_wide= age_vs_comp.pivot(columns="age_first_code_cud", values="converted_comp")
  
pingouin.mwu(x=age_vs_comp_wide["child"],y=age_vs_comp_wide["adult"], alternative="greater" )

# Kruskal-Wallis test is like Anova
pingouin.kruskal(data=a, dv="converte", between="job")



#EXAMPLE
# Select the weight_kilograms and late columns
weight_vs_late = late_shipments[["weight_kilograms", "late"]]

# Convert weight_vs_late into wide format
weight_vs_late_wide = weight_vs_late.pivot(columns="late", 
                                           values="weight_kilograms")


# Run a two-sided Wilcoxon-Mann-Whitney test on weight_kilograms vs. late
wmw_test = pingouin.mwu(x=weight_vs_late_wide["No"],
y=weight_vs_late_wide["Yes"], alternative="two-sided")



# Print the test results
print(wmw_test)


